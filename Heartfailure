package global.skymind.Sample;

import org.datavec.api.records.reader.RecordReader;
import org.datavec.api.records.reader.impl.collection.CollectionRecordReader;
import org.datavec.api.records.reader.impl.csv.CSVRecordReader;
import org.datavec.api.split.FileSplit;
import org.datavec.api.transform.TransformProcess;
import org.datavec.api.transform.filter.FilterInvalidValues;
import org.datavec.api.transform.schema.Schema;
import org.datavec.api.writable.Writable;
import org.datavec.local.transforms.LocalTransformExecutor;
import org.deeplearning4j.core.storage.StatsStorage;
import org.deeplearning4j.datasets.datavec.RecordReaderDataSetIterator;
import org.deeplearning4j.nn.conf.MultiLayerConfiguration;
import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
import org.deeplearning4j.nn.conf.layers.DenseLayer;
import org.deeplearning4j.nn.conf.layers.OutputLayer;
import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;
import org.deeplearning4j.nn.weights.WeightInit;
import org.deeplearning4j.optimize.listeners.ScoreIterationListener;
import org.deeplearning4j.ui.api.UIServer;
import org.deeplearning4j.ui.model.stats.StatsListener;
import org.deeplearning4j.ui.model.storage.InMemoryStatsStorage;
import org.nd4j.common.io.ClassPathResource;
import org.nd4j.evaluation.classification.Evaluation;
import org.nd4j.evaluation.regression.RegressionEvaluation;
import org.nd4j.linalg.activations.Activation;
import org.nd4j.linalg.dataset.SplitTestAndTrain;
import org.nd4j.linalg.dataset.ViewIterator;
import org.nd4j.linalg.dataset.DataSet;
import org.nd4j.linalg.dataset.api.iterator.DataSetIterator;
import org.nd4j.linalg.learning.config.Adam;
import org.nd4j.linalg.learning.config.Nesterovs;
import org.nd4j.linalg.lossfunctions.LossFunctions;
import org.nd4j.linalg.lossfunctions.impl.LossMCXENT;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

public class Heartfailure {
    private static Logger log = LoggerFactory.getLogger("Hearfailure.class");
    private static final int seed = 123;
    private static final double learningRate = 0.001;
    private static final int nEpochs = 5000;
    private static final int batchSize = 256;
    public static void main(String[] args) throws IOException, InterruptedException {

        File datafile = new ClassPathResource("Heartfailure/heart.csv").getFile();
        FileSplit filesplit = new FileSplit(datafile);
        RecordReader rr = new CSVRecordReader(1, ',');
        rr.initialize(filesplit);

        Schema schema = new Schema.Builder()
                .addColumnInteger("Age")
                .addColumnCategorical("Sex", Arrays.asList("M", "F"))
                .addColumnCategorical("ChestPainType", Arrays.asList("TA", "ATA", "NAP", "ASY"))
                .addColumnsInteger("RestingBP ", "Cholesterol", "FastingBS")
                .addColumnCategorical("RestingECG", Arrays.asList("Normal", "ST", "LVH"))
                .addColumnInteger("MaxHR")
                .addColumnCategorical("ExerciseAngina", Arrays.asList("Y", "N"))
                .addColumnDouble("Oldpeak")
                .addColumnCategorical("ST_Slope ", Arrays.asList("Up", "Flat"))
                .addColumnInteger("HeartDisease")
                .build();
        TransformProcess tp = new TransformProcess.Builder(schema)
                .filter(new FilterInvalidValues())
                .categoricalToInteger("ChestPainType", "RestingECG", "Sex", "ExerciseAngina", "ST_Slope ")
                .build();

        //  adding the original data to a list for later transform purpose
        List<List<Writable>> originalData = new ArrayList<>();

        while (rr.hasNext()) {
            originalData.add(rr.next());
        }
        System.out.println(tp.getFinalSchema());
        List<List<Writable>> transformed = LocalTransformExecutor.execute(originalData, tp);
        CollectionRecordReader crr = new CollectionRecordReader(transformed);
        DataSetIterator datasetiterator = new RecordReaderDataSetIterator(crr, transformed.size(),11,2);

        DataSet alldata = datasetiterator.next();
        alldata.shuffle();

        SplitTestAndTrain testandtrain = alldata.splitTestAndTrain(0.8);
        DataSet train = testandtrain.getTrain();
        DataSet test = testandtrain.getTest();

        //If neede add Kfold iteration here

//        NormalizerMinMaxScaler scaler = new NormalizerMinMaxScaler();
//        scaler.fit(train);
//        scaler.fit(test);
//        scaler.transform(train);
//        scaler.transform(test);

        //  Assigning dataset iterator for training purpose
        ViewIterator trainIter = new ViewIterator(train, batchSize);
        ViewIterator testIter = new ViewIterator(test, batchSize);

        //  Configuring the structure of the NN
        MultiLayerConfiguration config = new NeuralNetConfiguration.Builder()
                .seed(seed)
                .weightInit(WeightInit.XAVIER)
                .updater(new Nesterovs(learningRate, Nesterovs.DEFAULT_NESTEROV_MOMENTUM))
                .l2(0.001)
                .list()
                .layer(0, new DenseLayer.Builder()
                        .nIn(trainIter.inputColumns())
                        .nOut(10)
                        .activation(Activation.RELU)
                        .build())
                .layer(1, new DenseLayer.Builder()
                        .nIn(10)
                        .nOut(20)
                        .activation(Activation.RELU)
                        .build())
                .layer(2, new DenseLayer.Builder()
                        .nIn(20)
                        .nOut(30)
                        .activation(Activation.RELU)
                        .build())
                .layer(3, new DenseLayer.Builder()
                        .nIn(30)
                        .nOut(40)
                        .activation(Activation.RELU)
                        .build())
                .layer(4, new OutputLayer.Builder()
                        .nIn(40)
                        .nOut(trainIter.totalOutcomes())
                        .lossFunction(LossFunctions.LossFunction.XENT)
                        .activation(Activation.SIGMOID)
                        .build())
                .build();


        MultiLayerNetwork model = new MultiLayerNetwork(config);
        model.init();

        UIServer uiServer=UIServer.getInstance();
        StatsStorage storage=new InMemoryStatsStorage();
        uiServer.attach(storage);

        model.setListeners(new ScoreIterationListener(5),new StatsListener(storage,10));

        Evaluation eval;
        for(int i=0; i < nEpochs; i++) {
            model.fit(trainIter);

        }
        Evaluation evalTrain = model.evaluate(trainIter);
        Evaluation evalTest = model.evaluate(testIter);
        System.out.print("Train Data");
        System.out.println(evalTrain.stats());
        System.out.println(evalTrain.accuracy());

        System.out.print("Test Data");
        System.out.print(evalTest.stats());
        System.out.println(evalTest.accuracy());

    }
}
